{
    // Use IntelliSense to learn about possible attributes.
    // Hover to view descriptions of existing attributes.
    // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387
    "version": "0.2.0",
    "configurations": [
        {
            "name": "Python Debugger: icv_inference",
            "type": "debugpy",
            "request": "launch",
            "program": "${file}",
            "console": "integratedTerminal",
            "justMyCode": false,
            "args": [
            "--dataset", "hh_rlhf",
            "--model_type", "llama-2",
            "--model_size", "7b",
            "--start_id", "0",
            "--end_id", "10",
            "--reward_types", "hh_rlhf_helpful",
            "--generate_woICV", "True",
            ]
        },
        {
            "name": "Python Debugger: no args",
            "type": "debugpy",
            "request": "launch",
            "program": "${file}",
            "console": "integratedTerminal",
            "justMyCode": false,
        },
        {
            "name": "dpo",
            "type": "debugpy",
            "request": "launch",
            "program": "${file}",
            "console": "integratedTerminal",
            "justMyCode": false,
            "args": [
                "--dataset_name",
                "tatsu-lab/alpaca",
                "--output_dir",
                "./lorra_tqa_7b",
                "--user_tag",
                "[INST]",
                "--assistant_tag",
                "[/INST]",
                "--pos_type",
                "a truthful",
                "--do_eval",
                "--neg_type",
                "an untruthful",
                "--target_layers",
                "10,12,14,16,18,20",
                "--control_template",
                "Give {type} answer.",
                "--evaluate_nums",
                "24",
                "--model_name_or_path",
                "/scratch/prj/lmrep/llama2_model/Llama-2-7b-chat-hf",
                "--reward_types", "paraphrase",
                "--lorra_alpha", "5",
                "--lorra_beta", "0",
                "--lora_r","8",
                "--lora_alpha", "16",
                "--lora_dropout", "0.05",
                "--output_dir", "./results/lorra_inference/",
                "--overwrite_output_dir",
                "--num_train_epochs", "10",
                "--bf16", "True",
                "--per_device_train_batch_size", "16",
                "--per_device_eval_batch_size", "8",
                "--gradient_accumulation_steps", "1",
                "--evaluation_strategy", "steps",
                "--eval_steps", "10",
                "--save_strategy", "steps",
                "--save_steps", "10",
                "--learning_rate", "3e-4",
                "--weight_decay", "0.",
                "--lr_scheduler_type", "constant",
                "--logging_strategy", "steps",
                "--logging_steps", "100",
                "--tf32", "True",
                "--model_max_length", "128",
                "--q_lora", "False",
                "--gradient_checkpointing", "True",
                "--report_to", "wandb"
            ]
        }
    ]
}