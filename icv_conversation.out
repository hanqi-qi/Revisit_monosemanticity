nohup: ignoring input
2024-03-17 16:43:01.043416: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-17 16:43:01.374237: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.
2024-03-17 16:43:02.108498: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-17 16:43:05.828312: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
/scratch/prj/inf_llmcache/hf_cache/models--meta-llama--Llama-2-7b-chat-hf/snapshots/c1b0db933684edbfe29a06fa47eb19cc48025e93
bin /users/k2364214/.conda/envs/llm3/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:13<00:13, 13.93s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:18<00:00,  8.46s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:18<00:00,  9.28s/it]
/users/k2364214/.conda/envs/llm3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/users/k2364214/.conda/envs/llm3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Model loaded: llama-2 7b
Load data from 5 to 1000
All ICV vectors have been removed!

Results have been saved to results/llama-2-7b/toxicity/icv_noIntervention_conversation_toxicity.csv!
Load reward model: cooperleong00/deberta-v3-large_toxicity-scorer
Evaluate reward type: toxicity
  0%|          | 0/995 [00:00<?, ?it/s]100%|██████████| 995/995 [00:00<00:00, 2132515.32it/s]
/users/k2364214/.conda/envs/llm3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/users/k2364214/.conda/envs/llm3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The avg score for NEUTRAL is 0.7518140504707643
All ICV vectors have been removed!

ICV vectors have been added!

Evaluation reward model(s) on ['toxicity']
Results have been saved to results/llama-2-7b/toxicity/icv_conversation_toxicity.csv!
Load reward model: cooperleong00/deberta-v3-large_toxicity-scorer
Evaluate reward type: toxicity
  0%|          | 0/995 [00:00<?, ?it/s]100%|██████████| 995/995 [00:00<00:00, 2004482.46it/s]
The avg score for NEUTRAL is 0.8127579789065835
